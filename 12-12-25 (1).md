# Smart Seva: Advanced Fraud Detection Features

This document details the newly implemented **AI-Driven Fraud Detection Modules**. These modules go beyond simple rule matching by leveraging LLMs (Large Language Models) and Statistical methods to detect sophisticated forgery attempts.

---

## 1. Geospatial Analysis (LLM-Based)
**Module**: [geospatial_analysis.py](file:///c:/Users/sudik/OneDrive/Desktop/Mindcres/ai-space/smart_seva/server/src/services/modules/fraud_detection/geospatial_analysis.py)

### How It Works
Instead of using a static database of zip codes, we use the LLM's vast knowledge of geography to validate the relationship between the **Applicant's Address** and the **Issuing Authority**.

**Key Checks:**
1.  **Administrative Hierarchy**: Does the specific office (e.g., *Mandal Revenue Office, Ghatkesar*) actually have jurisdiction over the applicant's locality (e.g., *Rampally, Medchal District*)?
2.  **Distance Plausibility**: Is it physically plausible for the applicant to visit this office? (e.g., A resident of *Hyderabad* shouldn't likely get a certificate from a rural office in *Adilabad* 300km away).

### Example Scenario: Jurisdiction Mismatch
**Input Data:**
```json
{
  "address": "H.No 1-2-3, Hyderabad, Telangana",
  "issuing_authority": "Tahsildar Office, Mumbai, Maharashtra"
}
```

**System Logic:**
The LLM recognizes that *Mumbai, Maharashtra* has no administrative authority over a resident of *Hyderabad, Telangana*.

**Expected Output (JSON):**
```json
"geospatial_analysis": {
  "score": 85.0,  // High Fraud Risk
  "status": "mismatch",
  "details": {
    "llm_analysis": {
      "reasoning": "The applicant resides in Hyderabad (Telangana), but the certificate is issued by the Tahsildar in Mumbai (Maharashtra). This is a clear cross-state jurisdiction mismatch.",
      "is_consistent": false,
      "match_level": "Mismatch",
      "distance_plausibility": "Low"
    },
    "flags": [
      "Jurisdiction Mismatch (Mismatch): The applicant resides in Hyderabad..."
    ]
  }
}
```

---

## 2. Text Consistency Validator (NLP)
**Module**: [text_consistency_analysis.py](file:///c:/Users/sudik/OneDrive/Desktop/Mindcres/ai-space/smart_seva/server/src/services/modules/fraud_detection/text_consistency_analysis.py)

### How It Works
This module analyzes the *raw OCR text* to find linguistic anomalies that suggest forgery. Real government documents use specific formal language. Fake documents often have "tells" like chat-style language, bad grammar, or weird mixing of languages.

**Key Checks:**
1.  **Tone Analysis**: Is the language Formal/Official? (e.g., "This is to certify..." vs "Here is the cert for...")
2.  **Grammar & Spelling**: Checks for errors that wouldn't pass official proofreading.
3.  **OCR Tolerance**: Distinguishes between scan noise (e.g., `C0mputer` vs `Computer`) and actual typos (`recieve` vs `receive`).

### Example Scenario: Poorly Edited Fake
**Input Text (OCR):**
*"This is certifiy that Mr. Ravi Kumar is living in hyd. He is good person."*

**System Logic:**
The LLM detects:
-   Typo: "certifiy"
-   Informal Abbreviation: "hyd" instead of "Hyderabad"
-   Subjective/Informal Phrase: "He is good person" (Never appears in income/caste certs).

**Expected Output (JSON):**
```json
"text_consistency_analysis": {
  "score": 40.0, // Low Consistency = High Risk
  "status": "success",
  "details": {
    "issues": [
      "[Grammar] Misspelling 'certifiy'",
      "[Tone] Informal phrase 'He is good person'",
      "[Format] Abbreviation 'hyd' used in formal context"
    ],
    "tone": "Informal",
    "grammar_issues": ["certifiy"],
    "formatting_issues": ["hyd"]
  }
}
```

---

## 3. Integration & Final Scoring
**File**: [fraud_detection_node.py](file:///c:/Users/sudik/OneDrive/Desktop/Mindcres/ai-space/smart_seva/server/src/services/nodes/fraud_detection_node.py)

The system aggregates scores from all modules (Pattern, Statistical, Geospatial, Text Consistency) and takes the **Maximum Risk Score** as the `overall_score`.

**Logic:**
```python
overall_score = max(
    pattern_risk,
    statistical_risk,
    geospatial_risk,
    (100 - text_consistency_score) // Inverted because low consistency = high risk
)
```

If `overall_score > 80`: **REJECTED**
If `overall_score > 50`: **MANUAL_REVIEW**

---

## How to Test
Run the real images test script to see these modules in action on your local validation set:

```bash
uv run python -m test_scripts.test_real_images
```
